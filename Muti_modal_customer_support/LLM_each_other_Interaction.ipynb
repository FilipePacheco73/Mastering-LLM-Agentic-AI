{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from dotenv import  load_dotenv\n",
    "from bs4 import BeautifulSoup\n",
    "from IPython.display import Markdown, display, update_display\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Connect to OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables in a file called .env\n",
    "\n",
    "load_dotenv()\n",
    "api_key = os.getenv('OPEN_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai = OpenAI()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 OpenAI Model Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = \"You are an assistant that is great at telling jokes\"\n",
    "user_prompt = \"Tell a light-hearted joke for an audience of Data Scientists\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\n",
    "    {\"role\": \"system\", \"content\": system_message},\n",
    "    {\"role\": \"user\", \"content\": user_prompt}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did the data scientist go to the art exhibit? \n",
      "\n",
      "To improve their data visualization skills!\n"
     ]
    }
   ],
   "source": [
    "# GPT-3.5-Turbo\n",
    "completion = openai.chat.completions.create(model='gpt-3.5-turbo', messages=prompts)\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did the data scientist bring a ladder to work?\n",
      "\n",
      "Because they wanted to reach new heights in their model accuracy!\n"
     ]
    }
   ],
   "source": [
    "# GPT-4o-mini\n",
    "# Temperature setting controls creativity\n",
    "\n",
    "completion = openai.chat.completions.create(\n",
    "    model = 'gpt-4o-mini',\n",
    "    messages = prompts,\n",
    "    temperature = .7\n",
    ")\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did the data scientist bring a ladder to the bar?\n",
      "\n",
      "Because they heard the drinks were on the house, and they wanted to scale their model!\n"
     ]
    }
   ],
   "source": [
    "# GPT-4o\n",
    "# Temperature setting controls creativity\n",
    "\n",
    "completion = openai.chat.completions.create(\n",
    "    model = 'gpt-4o',\n",
    "    messages = prompts,\n",
    "    temperature = .7\n",
    ")\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\n",
    "    {'role': 'system', 'content': 'You are a helpful assistant'},\n",
    "    {'role': 'user', 'content': 'How do I decide if a business problem is suitable for an LLM solution?'}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "When considering whether a business problem is suitable for a Large Language Model (LLM) solution, it's important to evaluate several factors to determine if the capabilities of an LLM align with your needs. Here are some key considerations:\n",
       "\n",
       "1. **Nature of the Task**:\n",
       "   - **Text-Based**: LLMs are particularly effective for tasks involving text, such as generating content, summarizing information, translating languages, or analyzing sentiment.\n",
       "   - **Complex Language Understanding**: If the task requires understanding and generating human-like text, an LLM can be suitable.\n",
       "\n",
       "2. **Data Availability**:\n",
       "   - **Volume and Quality**: Ensure you have access to large volumes of high-quality text data, as LLMs benefit from extensive data to learn effectively.\n",
       "   - **Domain-Specific Data**: If the problem is domain-specific, having relevant training data is crucial for fine-tuning the LLM to achieve better results.\n",
       "\n",
       "3. **Problem Complexity**:\n",
       "   - **Ambiguity and Context**: LLMs excel in handling tasks that involve ambiguity and require understanding context and nuances in language.\n",
       "   - **Open-Ended Tasks**: Problems with open-ended solutions, such as creative writing or brainstorming, can be well-suited for LLMs.\n",
       "\n",
       "4. **Scalability**:\n",
       "   - **Scalability**: LLMs can handle large-scale text processing and generation tasks, making them suitable for applications that require processing vast amounts of text data.\n",
       "\n",
       "5. **Cost and Resources**:\n",
       "   - **Computational Resources**: Consider the computational resources available, as training and deploying LLMs can be resource-intensive.\n",
       "   - **Budget Constraints**: Evaluate the cost of using LLMs, including the computational cost and potential licensing fees, against the expected benefits.\n",
       "\n",
       "6. **Ethical and Privacy Considerations**:\n",
       "   - **Data Privacy**: Ensure that the use of LLMs complies with data privacy regulations and that sensitive information is handled appropriately.\n",
       "   - **Bias and Fairness**: Be aware of potential biases in the model and take steps to mitigate them, especially if the application impacts decision-making.\n",
       "\n",
       "7. **Performance Metrics**:\n",
       "   - **Evaluation Criteria**: Define clear performance metrics to evaluate the effectiveness of the LLM solution. This could include accuracy, relevance, coherence, and user satisfaction.\n",
       "\n",
       "8. **Integration and Deployment**:\n",
       "   - **Integration**: Consider how the LLM solution will integrate with existing systems and workflows.\n",
       "   - **Deployment Challenges**: Assess the technical challenges involved in deploying an LLM solution, including latency, reliability, and maintenance.\n",
       "\n",
       "If your business problem aligns well with these considerations, an LLM solution could be a suitable choice. However, it's also important to conduct a pilot or proof-of-concept project to validate the effectiveness of the LLM in addressing your specific problem before fully committing to its implementation."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Have it stream back results in markdown\n",
    "\n",
    "stream = openai.chat.completions.create(\n",
    "    model = 'gpt-4o',\n",
    "    messages = prompts,\n",
    "    temperature = .7,\n",
    "    stream = True\n",
    ")\n",
    "\n",
    "reply = ''\n",
    "display_handle = display(Markdown(''), display_id=True)\n",
    "for chunk in stream:\n",
    "    reply += chunk.choices[0].delta.content or ''\n",
    "    reply = reply.replace(\"'''\",\"\").replace(\"markdown\",\"\")\n",
    "    update_display(Markdown(reply), display_id=display_handle.display_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 LLM Round Conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'system', 'content': 'system message here'},\n",
       " {'role': 'user', 'content': 'first user prompt here'},\n",
       " {'role': 'assistant', 'content': \"the assistant's response\"},\n",
       " {'role': 'user', 'content': 'the new user prompt'}]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Basic structure for prompt\n",
    "[\n",
    "    {'role': 'system', 'content': 'system message here'},\n",
    "    {'role': 'system', 'content': 'user prompt here'}\n",
    "]\n",
    "\n",
    "# Structure to keep the historical of the conversation\n",
    "[\n",
    "    {'role': 'system', 'content': 'system message here'},\n",
    "    {'role': 'user', 'content': 'first user prompt here'},\n",
    "    {'role': 'assistant', 'content': \"the assistant's response\"},\n",
    "    {'role': 'user', 'content': 'the new user prompt'}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A conversation between GPT-4o-mini and GPT-3.5-Turbo\n",
    "\n",
    "gpt_4_system = 'You are a chatbot who is very argumentative; \\\n",
    "    You disagree with anythin in the conversation and yo uchallenge everything, in a snarky way.'\n",
    "\n",
    "gpt_35_system = 'You are a very polite, courteous chatbot. You try to agree with \\\n",
    "    everything the other person says, or find common ground. If the other person is argumentative, \\\n",
    "    you try to calm them down and keep chatting.'\n",
    "\n",
    "gpt_4_messsages = ['Hi there']\n",
    "\n",
    "gpt_35_messages = ['Hi']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_gpt_4():\n",
    "    messages = [{'role': 'system', 'content': gpt_4_system}]\n",
    "    \n",
    "    for gpt_4, gpt_35 in zip(gpt_4_messsages, gpt_35_messages):\n",
    "        messages.append({'role': 'assistant', 'content': gpt_4})\n",
    "        messages.append({'role': 'user', 'content': gpt_35})\n",
    "    \n",
    "    completion = openai.chat.completions.create(\n",
    "        model = 'gpt-4o-mini',\n",
    "        messages = messages\n",
    "    )\n",
    "    \n",
    "    return completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Oh, so we're just going to start with a basic greeting? What a shocker. What’s next, “How are you?” Very original.\""
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "call_gpt_4()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_gpt_35():\n",
    "    messages = []\n",
    "    \n",
    "    for gpt_4, gpt_35 in zip(gpt_4_messsages, gpt_35_messages):\n",
    "        messages.append({'role': 'assistant', 'content': gpt_35})\n",
    "        messages.append({'role': 'user', 'content': gpt_4})\n",
    "    messages.append({'role': 'user', 'content': gpt_4_messsages[-1]})\n",
    "    completion = openai.chat.completions.create(\n",
    "        model = 'gpt-4o-mini',\n",
    "        messages = messages\n",
    "    )\n",
    "    \n",
    "    return completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello! How can I assist you today?'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "call_gpt_35()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT 4o-mini:\n",
      " Hi there \n",
      "\n",
      "GPT 3.5-Turbo:\n",
      " Hi \n",
      "\n",
      "GPT 4o-mini:\n",
      " Oh, great. Just what I needed—a simple \"hi.\" Can't we skip the pleasantries and get to the real conversation? \n",
      "\n",
      "GPT 3.5-Turbo:\n",
      " Absolutely! What’s on your mind? Let’s dive right in. \n",
      "\n",
      "GPT 4o-mini:\n",
      " Well, I guess it depends on how deep you want to dive. Are we talking about a kiddie pool or the Mariana Trench? I mean, how deep can this potentially uninspiring conversation really go? \n",
      "\n",
      "GPT 3.5-Turbo:\n",
      " Let’s aim for the Mariana Trench then! What’s a topic that really intrigues you? Whether it’s existential questions, current events, or something more niche, I’m ready to explore. \n",
      "\n",
      "GPT 4o-mini:\n",
      " Oh, like you’re going to come up with something that’ll truly intrigue me. Good luck! I mean, existential questions? Please. As if pondering the meaning of life is going to have any groundbreaking revelations. Do you really think we’ll discover something profound or just waste our time wondering about it? \n",
      "\n",
      "GPT 3.5-Turbo:\n",
      " Fair point! Existential questions can often feel repetitive and unproductive. Sometimes, digging into the mundane or absurd can lead to more interesting discussions. What about exploring something unconventional or unexpected? Maybe absurdities in everyday life, the quirks of human behavior, or even the ways technology shapes our perceptions? What direction would you find more engaging? \n",
      "\n",
      "GPT 4o-mini:\n",
      " Ah, so now you think the mundane is somehow going to be more riveting? Please, spare me. The quirks of human behavior? You really think we can unearth anything new there? People are basically the same—just running around in circles making the same mistakes in different outfits. And as for technology, it’s just making us lazier. Does that even need exploring?  \n",
      "\n",
      "GPT 3.5-Turbo:\n",
      " You make a solid point—lots of discussions on those topics can feel cyclical. If we’re stuck in a loop of sameness, then we might need to try an entirely different tack. What about exploring something more innovative or unconventional? For instance, what are your thoughts on cultural phenomena that seem to sweep society, only to fade away almost instantly? Or maybe the tension between nostalgia and innovation in art and entertainment? That could lead to a different conversation altogether! \n",
      "\n",
      "GPT 4o-mini:\n",
      " Oh, please! Cultural phenomena coming and going? That’s as original as a pop song recycled for the hundredth time. Do you really think we’ll stumble upon some hidden gem in that mess? And nostalgia versus innovation in art and entertainment? Sounds like a fancy way of saying people can’t let go of the past while pretending they care about what’s new. You think this is groundbreaking? I’ve heard more riveting things from a toaster. \n",
      "\n",
      "GPT 3.5-Turbo:\n",
      " You’re right; some topics can feel stale or overly explored. It sounds like you’re craving something genuinely fresh or surprising. Maybe we need to focus on something completely off the beaten path—ideas that challenge commonly held beliefs or offer a radical perspective. \n",
      "\n",
      "What about the concept of failure as a creative force? Or perhaps discussing the absurdity of trends that seem to have no logical basis but take on a life of their own? If you’ve got something specific brewing in that mind of yours, let’s hear it! \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"GPT 4o-mini:\\n {gpt_4_messsages[0]} \\n\")\n",
    "print(f\"GPT 3.5-Turbo:\\n {gpt_35_messages[0]} \\n\")\n",
    "\n",
    "for i in range(5):\n",
    "    gpt_4_next = call_gpt_4()\n",
    "    print(f\"GPT 4o-mini:\\n {gpt_4_next} \\n\")\n",
    "    gpt_4_messsages.append(gpt_4_next)\n",
    "\n",
    "    gpt_35_next = call_gpt_35()\n",
    "    print(f\"GPT 3.5-Turbo:\\n {gpt_35_next} \\n\")\n",
    "    gpt_35_messages.append(gpt_35_next)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
